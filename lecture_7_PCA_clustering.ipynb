{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups, load_digits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# these are new imports for dimensionality reduction\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "# these are new imports for clustering\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R package [`cluster.datasets`](http://cran.r-project.org/web/packages/cluster.datasets/cluster.datasets.pdf) has some good datasets for experimenting with unsupervised learning techniques like dimensionality reduction and clustering.  Here, we'll use the `cake.ingredients.1961` dataset of cake recipes, which I've exported to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the cakes data, and take a look\n",
    "cakes = pd.read_csv(\"./cakes.csv\")\n",
    "cakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a cake recipe, and each column is an ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's remove leading and trailing spaces from the 'Cake' column to clean up names of cakes\n",
    "cakes[\"Cake\"] = cakes.Cake.str.strip() # Remove leading\n",
    "cakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store a dictionary of the ingredient abbreviations so we can look them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingredients_dict = {\n",
    "    \"AE\": \"Almond essence\",\n",
    "    \"BM\": \"Buttermilk\",\n",
    "    \"BP\": \"Baking powder\",\n",
    "    \"BR\": \"Butter\",\n",
    "    \"BS\": \"Bananas\",\n",
    "    \"CA\": \"Cocoa\",\n",
    "    \"CC\": \"Cottage Cheese\",\n",
    "    \"CE\": \"Chocolate\",\n",
    "    \"CI\": \"Crushed Ice\",\n",
    "    \"CS\": \"Crumbs\",\n",
    "    \"CT\": \"Cream of tartar\",\n",
    "    \"DC\": \"Dried currants\",\n",
    "    \"EG\": \"Eggs\",\n",
    "    \"EY\": \"Egg white\",\n",
    "    \"EW\": \"Egg yolk\",\n",
    "    \"FR\": \"Sifted flour\",\n",
    "    \"GN\": \"Gelatin\",\n",
    "    \"HC\": \"Heavy cream\",\n",
    "    \"LJ\": \"Lemon juice\",\n",
    "    \"LR\": \"Lemon\",\n",
    "    \"MK\": \"Milk\",\n",
    "    \"NG\": \"Nutmeg\",\n",
    "    \"NS\": \"Nuts\",\n",
    "    \"RM\": \"Rum\",\n",
    "    \"SA\": \"Soda\",\n",
    "    \"SC\": \"Sour cream\",\n",
    "    \"SG\": \"Shortening\",\n",
    "    \"SR\": \"Granulated sugar\",\n",
    "    \"SS\": \"Strawberries\",\n",
    "    \"ST\": \"Salt\",\n",
    "    \"VE\": \"Vanilla extract\",\n",
    "    \"WR\": \"Water\",\n",
    "    \"YT\": \"Yeast\",\n",
    "    \"ZH\": \"Zwiebach\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the column of cake names so that we have a numeric only dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = cakes.iloc[:, 1:]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run a simple PCA using the [scikit-learn class](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).  If we don't specify `n_components` or set it to `None`, it will use the maximum number of principal components.  Note that the principal components are unique up to a sign, so on a small data set it makes sense to go ahead and get all the components. If you have a large data set, you may want to just get the first couple of components, then decide if you need more computation to get the remaining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the explained variance of each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Line plot of variance explained\n",
    "plt.plot(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_) \n",
    "\n",
    "# Add points to the plot\n",
    "plt.scatter(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_) \n",
    "\n",
    "# Axes labels\n",
    "plt.xlabel(\"Principal Components Number\")\n",
    "plt.ylabel(\"Percentage of Variance Explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, let's lot the cumulative variance explained\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(range(len(pca.explained_variance_ratio_)), cumsum)\n",
    "plt.scatter(range(len(pca.explained_variance_ratio_)), cumsum)\n",
    "plt.xlabel(\"Principal Components Number\")\n",
    "plt.ylabel(\"Cumulative Percentage of Variance Explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're looking for an \"elbow\", it looks like roughly 6 or 7 principal components would be enough.  To actually get each row transformed into the principal component space, we can call `transform` on an already fit `PCA` object, or we can do both at once with `fit_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll redo PCA, but only with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_trans = pca.fit_transform(X)\n",
    "X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column represents the **scores** of each observation for the first component, and the second column represents the scores for the second component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function for plotting. This function will let us plot the scores for each observation, and optionally the loadings for each of the components.  If we include the loadings, we end up with a *biplot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_PCA(pca, X, print_row_labels, row_labels, col_labels, biplot=False, y_scale=(None, None), font_size=None):\n",
    "    \n",
    "    # transform our data to PCA space\n",
    "    X_trans = pca.fit_transform(X)\n",
    "\n",
    "    # handle the scaling of the plot\n",
    "    xmin, xmax = min(X_trans[:, 0]), max(X_trans[:, 0])\n",
    "    if y_scale == (None, None): # use the data to determine the scale on the vertical axis.\n",
    "        ymin, ymax = min(X_trans[:, 1]), max(X_trans[:, 1])\n",
    "        xpad, ypad = 5, 5\n",
    "    else: # Otherwise, use the vertical scale passed into the function\n",
    "        ymin, ymax = y_scale\n",
    "        xpad, ypad = 5, 1\n",
    "        \n",
    "    plt.xlim(xmin - xpad, xmax + xpad) # Set the horizontal limits\n",
    "    plt.ylim(ymin - ypad, ymax + ypad) # Set the vertical limits\n",
    "\n",
    "    # plot words instead of points\n",
    "    # We use 'zip' to create a collecton of tuples, one per observation, that\n",
    "    # collects information on that tuple -score for first prin. component, score for second component, and text label\n",
    "    if print_row_labels:\n",
    "        for x, y, label in zip(X_trans[:, 0], X_trans[:, 1], row_labels):\n",
    "            if font_size is None:\n",
    "                plt.text(x, y, label)\n",
    "            else:\n",
    "                plt.text(x, y, label, size=font_size)\n",
    "    else:\n",
    "        for x, y in zip(X_trans[:, 0], X_trans[:, 1]):\n",
    "            plt.scatter(x, y)\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "\n",
    "    # if we want a biplot, get the loading and plot\n",
    "    # axes with labels\n",
    "    if biplot:\n",
    "        eigenvectors = pca.components_.transpose()\n",
    "        for i, col in enumerate(col_labels):\n",
    "            x, y = 10*eigenvectors[i][0], 10*eigenvectors[i][1]\n",
    "            plt.arrow(0, 0, x, y, color='r', width=0.002, head_width=0.05)\n",
    "            plt.text(x* 1.4, y * 1.4, col, color='r', ha='center', va='center')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's plot jsut the scores for each observation. \n",
    "plot_PCA(pca, X, True, cakes.Cake, X.columns, biplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not encouraging.  When we see something like this, it's typically a scaling issue.  Let's plot again, but include the loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_PCA(pca, X, True, cakes.Cake, X.columns, biplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we're influenced by two large outliers - 'One Bowl Chocolate' and 'Angel' cake.    The first principal component is dominated by \"cocoa\" and \"shortening\" because the \"One Bowl Chocolate\" cake has a huge amount of these.  The second principal component is dominated by \"egg whites\" because of the \"Angel\" foodcake recipe.\n",
    "\n",
    "To resolve this issue, let's first try mean-centering the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_scaled = scale(X, with_mean=True, with_std=False) # Mean-centering\n",
    "plot_PCA(pca, X_scaled, True, cakes.Cake, X.columns, biplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now both center and scale to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_scaled = scale(X, with_mean=True, with_std=True) # Center and scale\n",
    "plot_PCA(pca, X_scaled, True, cakes.Cake, X.columns, biplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a zoomed in version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_scaled = scale(X, with_mean=True, with_std=True)\n",
    "plot_PCA(pca, X_scaled, True, cakes.Cake, X.columns, biplot=True, y_scale=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To me, it looks like cheesecakes are off to the right on the first principal component, and the second principal component is quantifying whether the cake has fruit or not...but, this is subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch for a moment to dataset on handwritten digits.  Each observation is an 8 pixel by 8 pixel image, so each image is characterized by 64 pieces of information (color for each pixel).  We also get data on what each image represents (a digit between 0 and 9), though we won't use this for the purposes of unsupervised learning. We will use the target data to understand how well the PCA is doing for teaching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this will show us the pixel values\n",
    "image_num = 1000\n",
    "digits.images[image_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also see the labels for each images, i.e., the digit in each image.\n",
    "digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function to plot each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_handwritten_digit(the_image, label):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(the_image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Image: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generates the image, with the digit label listed in the title\n",
    "plot_handwritten_digit(digits.images[image_num], digits.target[image_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we'll scale the digits data and store the labels.\n",
    "digits_data = scale(digits.data)\n",
    "\n",
    "labels = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll run PCA on the digits data with two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "digits_trans = pca.fit_transform(digits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot of the first two principal components, colored and labeled by the true digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "for x, y, label in zip(digits_trans[:, 0], digits_trans[:, 1], labels):\n",
    "    plt.text(x, y, label, size=8, color=plt.cm.Set1(label/10.))\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the clearest view of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our goal is visualizing a high dimensional dataset, the [t-SNE](http://lvdmaaten.github.io/tsne/) algorithm usually does a superior job of finding structure in the high-dimensional data that can be visualized in two dimensions. t-SNE works by trying to find a lower dimensional embedding of the data such that observations near each other in lower-dimensional space were close to each other in higher dimensional space.  It is an example of a non-linear dimensionality reduction technique.  Another example of such a technique is a neural network autoencoder (which I'll touch on next lecture).\n",
    "\n",
    "t-SNE is often used in one of two ways.  First, you can apply it before clustering to get a good 2-d or 3-d representation of the data, *then* apply clustering to the 2-d or 3-d projection of the data.  Second, you can cluster the data first, then apply t-SNE just to get a better visualization of the data.\n",
    "\n",
    "There's a [scikit-learn class](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) for running t-SNE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=True)\n",
    "digits_trans = tsne.fit_transform(digits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Like PCA with two components, TSNE also describes each observation using two coordinates.\n",
    "# These are analogous to, but derived differently from, the first two PCA components\n",
    "digits_trans[0:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can again plot the data, this time using the two-dimensional projection\n",
    "# provided by TSNE\n",
    "\n",
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "#for x, y, label in zip(digits_trans[labels==6, 0], digits_trans[labels==6, 1], labels[labels==6]):\n",
    "for x, y, label in zip(digits_trans[0:1000, 0], digits_trans[0:1000, 1], labels[0:1000]):\n",
    "    plt.text(x, y, label, size=8, color=plt.cm.Set1(label/10.))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly does a better job at finding the \"structure\" in the high-dimensional dataset.  Notice that 3, 5, and 9 end up near each other.  But there are some 1's that are closer to the 2's, and some 9's that are closer to the 7's and the 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to cakes.  We'll use some functions from scipy to run hierarchical clustering.  `linkage` calculates the distances and linkages, and `dendrogram` displays the actual tree dendrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_single = linkage(scale(X), method='single', metric=\"euclidean\") # single, complete, average, and ward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_single, orientation=\"top\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ISLR says, single linkage tends to produce really unbalanced trees.  We can put the dendrogram on its side to make it easier to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_single, orientation=\"right\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_complete = linkage(scale(X), method='complete', metric=\"euclidean\") # single, complete, average, and ward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_complete, orientation=\"top\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_complete, orientation=\"right\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another linkage method is **Ward's Method**, which is slightly diffferent than the other methods. It decides which clusters to merge by, at each step, considering every possible merge of two clusters and using the one that least increases the total within-cluster variation.  This resembles the greedy approach taken by decision treess in it uses a greedy approach to minimzing an objective function that measures error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_ward = linkage(scale(X), method='ward', metric=\"euclidean\") # single, complete, average, and ward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_ward, orientation=\"top\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_ward, orientation=\"right\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering is doing something sensible: the cheesecakes group together and are on their own, the chocolate cakes are together (sour cream fudge, red devil's, sweet chocolate, and one bowl chocolate), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general resource, the [scikit-learn clustering page](http://scikit-learn.org/stable/modules/clustering.html) is great.  It has all the different kinds of clustering algorithms with their pros and cons.  Here, we'll focus on k-means for clustering the digits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init can be k-means++ or random; k-means++ is just a smarter version of random that forces the\n",
    "# centers to be further apart\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++', n_init=10, max_iter=300, verbose=True, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.fit(digits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the assigned cluster or label of each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cluster centers themselves.  Each centroid is a vector with 64 values -one per pixel in an 8x8 pixel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"inertia\" tells us the within cluster sum-of-squares, or the \"sum of distances of samples to their closest cluster center.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we make a plot where we color by the k-means label instead of the true label.  In order to get a two dimensional plot, we plot each observation in terms of the t-SNE projection from earlier in this notebook.  However, we've still clustered in the original 64-dimensional space -therefore, this is an example of using t-SNE to help visualize data that's already been clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "for x, y, true_label, kmeans_label in zip(digits_trans[:, 0], digits_trans[:, 1], labels, kmeans.labels_):\n",
    "    plt.text(x, y, true_label, size=8, color=plt.cm.Set1(kmeans_label/10.))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that things are decent, but definitely more confused than with the true labels.  Ideally, all there should only be one color per digit label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `predict` method, which will tell us which cluster center some new data is closest too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.predict(digits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method will transform data into the cluster distance space.  That is, how far the point is from each cluster center.  Hence, the resulting object ```transformed``` will have one row per observation, and one column per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed = kmeans.transform(digits_data)\n",
    "transformed[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large datasets, there's a much faster implementation of k-means called [mini-batch k-means](http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf), and a [scikit-learn class](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) for running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=100, init='k-means++', n_init=10, max_iter=300, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_kmeans.fit(digits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "for x, y, true_label, kmeans_label in zip(digits_trans[:, 0], digits_trans[:, 1], labels, mb_kmeans.labels_):\n",
    "    plt.text(x, y, true_label, size=8, color=plt.cm.Set1(kmeans_label/10.))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can re-cover the \"correct\" number of clusters using the silhouette statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = range(3, 70, 2)\n",
    "silhouette_stats = []\n",
    "for this_n_clusters in n_clusters:\n",
    "    print \"Fitting %s clusters...\" % this_n_clusters\n",
    "    kmeans = KMeans(n_clusters=this_n_clusters, init='k-means++', n_init=10, max_iter=300, verbose=False, n_jobs=1)\n",
    "    kmeans.fit(digits_data)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_stats.append(silhouette_score(digits_data, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(n_clusters, silhouette_stats)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
