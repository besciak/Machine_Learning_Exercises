{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we'll be looking at various kinds of neural networks.  Neural networks in python are a very quickly evolving area, and there are many different competing packages for working with them.  Unfortunately, there's not yet a standard set of packages in scikit-learn like we've seen for many other machine learning methods.  \n",
    "\n",
    "Most of the packages are high level wrappers around [Theano](http://deeplearning.net/software/theano/), which is a mathematical package for easily working with numerical expressions of arrays and matrices and their gradients.  Additionally, Theano code will also run seamlessly on a GPU if one is available.  This makes training much, much faster.  Here's an [ipython notebook](http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb) on Theano if you're interested. \n",
    "\n",
    "We're going to look at three packages.  The first is [scikit-neuralnetwork](https://github.com/aigamedev/scikit-neuralnetwork) (installation instructions are at this link as well).  It's interface is the simplest, but it doesn't appear to be as widely used, and it's unclear if this package will \"win\" the race or not.\n",
    "\n",
    "[Keras](https://github.com/fchollet/keras/) is a relatively new package.  It looks to be a good balance between sophistication and simplicity.  Installation instructions are at that link.\n",
    "\n",
    "We will also look at Google's tensorflow package at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'urlretrieve'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-13a6b4edc8bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'urlretrieve'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pickle lets us save python objects to a file and read them back in\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# here are our neural network imports\n",
    "#from sknn import mlp\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import theano\n",
    "\n",
    "from urllib import urlretrieve\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# scikit-learn does have a restricted boltzman machine class for doing unsupervised\n",
    "# feature learning\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where Theano will run our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print (theano.config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a GPU, we could use it by setting `theano.config.device='gpu'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron with a Single Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a standard dataset of handwritten digits.  Note that this is a much bigger, higher resolution dataset than the handwritten digits dataset that we've seein in previous lectures.  In this first example, we'll use scikit-neuralnetwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll download the MNIST dataset as a pickle file, save it to a local file, and then read in its contents.  If we've already downloaded the file, we'll just read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MNIST dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'urlretrieve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b29f371596a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_FILENAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloading MNIST dataset...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_URL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDATA_FILENAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_FILENAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urlretrieve' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "if not os.path.exists(DATA_FILENAME):\n",
    "    print (\"Downloading MNIST dataset...\")\n",
    "    urlretrieve(DATA_URL, DATA_FILENAME)\n",
    "\n",
    "with gzip.open(DATA_FILENAME, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle object has a training set, a validation set, and a test set.  Let's split those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = data[0]\n",
    "X_valid, y_valid = data[1]\n",
    "X_test, y_test = data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images of handwritten digits are 28 by 28 pixels (28*28=784):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response varaible is a label ranging between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous handwritten digits dataset that we worked with was only 8 by 8 pixels.  Let's define a function so that we can look at some of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_handwritten_digit(the_image, label):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(the_image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_num = 1220\n",
    "plot_handwritten_digit(X_train[image_num].reshape((28, 28)), y_train[image_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define some constants that will be used in the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of units in a single hidden layer\n",
    "NUM_HIDDEN_UNITS = 512\n",
    "\n",
    "# these parameters control the gradient descent process to learn the weights\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9 \n",
    "\n",
    "# we'll feed in this many training examples at a time (for stochastic gradient descent)\n",
    "BATCH_SIZE = 600\n",
    "\n",
    "# this is how many times we'll go through the set of batches, i.e. a full pass over\n",
    "# all of the training data\n",
    "NUM_EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit-neuralnetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a network with scikit-neuralnetwork because it's by far the simplest.  Unfortunately, it looks like this package only supports squared loss and not cross-entropy for classification problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sknn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-04eaa6bb5f54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msknn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'sknn'"
     ]
    }
   ],
   "source": [
    "import sknn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we'll specify the layers. We'll build a simple network with an input layer, \n",
    "# one hidden layer that uses sigmoid activation, and one output layer that uses\n",
    "# softmax activation.  We're using softmax for the output layer since we're doing multi-class classification; softmax\n",
    "# rescales the output layer so that the values of the output nodes are all positive and sum to 1 (hence, they\n",
    "# can be viewed as a probability distribution).\n",
    "\n",
    "# The 'mlp' you see throughout corresponds to 'multi-layer perceptron'\n",
    "layers = [mlp.Layer(\"Sigmoid\", units=NUM_HIDDEN_UNITS), mlp.Layer(\"Softmax\")]\n",
    "\n",
    "# Second, we create the model, much as we did for models earlier in the course\n",
    "# This model contains all the settings and hyperparameters required to train the network. \n",
    "# It does not actally refer to our training data.  \n",
    "# Notice also that we're using the mlp.Classifier class, since we're doing classifiation.\n",
    "# With this information, sknn will know that our output consists of a set of labels, and so\n",
    "# we don't need to vectorize the response data.\n",
    "sknn_mlp = mlp.Classifier(loss_type=\"mse\", batch_size=BATCH_SIZE, layers=layers, learning_rate=LEARNING_RATE, \n",
    "                        learning_rule=\"nesterov\", learning_momentum=MOMENTUM, n_iter=NUM_EPOCHS, verbose=True)\n",
    "\n",
    "# We can also get a summary of the model's settings.\n",
    "sknn_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll fit it and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sknn_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can predict on the test data using the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = sknn_mlp.predict(X_test)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the network's performance, using the same classification accuracy metrics we used in earlier lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, test_preds)\n",
    "print accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sknn is easy to use, but not extremely popular.  On the other hand, keras is quite popular.  Interestingly, it does not actually do computations -it provides a relatively easy to use syntax for setting up and training neural networks that can be combined with a computational backend (theano, tensorflow).  This makes it very convenient since the keras code you write when using a theano backend is the same as the code you'd write if you were using tensorflow as a backend.  Swapping between theano and tensorflow on the backend requires a simple change to the keras configuration file (should be located in the ```.keras``` folder in your root directory, and called ```keras.json```; see https://keras.io/backend/).\n",
    "\n",
    "For a list of keras commands, and references on what each does, see: https://keras.io/layers/core/.\n",
    "\n",
    "Thes examples are based on the example found at: \n",
    "https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "Be careful with keras examples -you have to go to the actual Github\n",
    "respository to get examples that are consistent with the latest version\n",
    "of the package, otherwise everything breaks...\n",
    "\n",
    "First, we setup the model.  This model is again simple - one layer for inputs, one hidden layer, and then one layer for outputs.  We will use sigmoid activation for the hidden layer, and softmax activation for the output layer.  This setup lets us perform classification -by taking the largest value in the softmax layer across the ten possible outputs (0, 1, ..., 9), we can classify new observations.  We will also measure our network's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tell keras we want to create a sequential (feed-forward network) model, in which one\n",
    "# layer follows the next\n",
    "model = Sequential()\n",
    "\n",
    "# Create the input layer and the hidden layer of the network\n",
    "# 'Dense' indicates that we want all inputs to connect to every node in the hidden layer\n",
    "# 'input_shape' tells the hidden layer the dimension of the input to expect, which is determined\n",
    "#    by our data (the number of predictors in our data set = 784 pixels per image)\n",
    "# 'NUM_HIDDEN_UNITS' (which we defined above) tells keras how many nodes are the hidden layer connected\n",
    "#    to the input layer\n",
    "# 'activation' specifies how values from the input node should be processed by hidden nodes.\n",
    "model.add(Dense(NUM_HIDDEN_UNITS, input_shape=(784,), activation='sigmoid', init='uniform'))\n",
    "\n",
    "# Next, we'll create an output layer.\n",
    "# The value '10' tells keras we want this layer to have ten nodes\n",
    "# The 'activation' tells keras we want to use the softmax function\n",
    "# Note that we don't specify 'input_shape'.  The input to this layer is the output of the hidden\n",
    "# layer we created above. keras is smart enough to figure this out, which is why we only need\n",
    "# to specify how many nodes are in the output layer.\n",
    "model.add(Dense(10, activation='softmax', init='uniform'))\n",
    "\n",
    "# Next, we specify the properties of our optimizer.\n",
    "# We'll be using stochastic gradient descent with momentum, along with \n",
    "# the crossentropy cost function.\n",
    "# Note: The 'decay' argument was not discussed in lecture - it reduces the learning\n",
    "# rate as we get further into training (e.g., as we go from one epoch to the next)\n",
    "# The benefit of decay is that the optimizer will make bigger adjustments\n",
    "# early on, then do fine-tuning later in the training process.\n",
    "sgd = SGD(lr=LEARNING_RATE, decay=1e-6, momentum=MOMENTUM, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Summarize the model setup\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will convert the training labels to vectors of dimension 10. Each vector only has one '1', in the index corresponding to the correct label.  For instance, if in an image is a '5', then there will be a '1' in the corresponding spot in the response vector, and 0's everywhere else.  This approach mirrors the fact that our output layer has 10 nodes, one per possible digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The np_utils module is part of keras, and has functioan designed to make it easier to work with neural networks.\n",
    "# Here, we're using it to convert the label response data into vectors.\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, let's fit the model.  We pass it the training data, the batch_size (for SGD), and the number of epochs.  Notice the ```.fit``` syntax, which is the same syntax used for all scikit-learn models we've used thusfar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, nb_epoch=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll predict on the test set using the neural network.  Since we had ten output nodes, keras will give us a 10-dimensional vector for each test observation.  Each entry in the vector can be interpreted as the probability that the test vector's label corresponds to the entry (e.g., first entry is the probability that the test observation is a `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_keras = model.predict(X_test)\n",
    "Y_test_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we'll use np_utils again to convert the 10-dimensional probability distribution for each test\n",
    "# observation into an actual label (by looking at the max)\n",
    "test_preds = np_utils.categorical_probas_to_classes(Y_test_keras)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we measure the accuracy of the neural network.  Recall that precision is the fraction of observations labeled as a class that actually are of that class, and recall is the fraction of observations from a class that were labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finaly we measure the accuracy of the neural network.\n",
    "print classification_report(y_test, test_preds)\n",
    "print accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to visualize what the hidden nodes are picking up on, we can extract the weights for a single hidden node, then pass those weights into the function we created earlier to plot digits.  This works because there is one weight per pixel, so the number of weights for a node can be rehaped into an 'image'.  When a pixel has a large weight, it will be darker in the image, and when a pixel has a low weight, it will lighter in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can extract the weights from the hidden layer using the get_weights\n",
    "# function provided by keras.\n",
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tell us that keras is tracking four sets of 'weights'.  The first set of weights, ```model.get_weights()[0]``` corresponds to the weights used when mapping the inputs to the first hidden layer.  The second set of weights, ```model.get_weights()[1]```, actually corresponds to the bias terms to the first hidden layers.  The second two entries, indexed by ```2``` and ```3``` are the weights and biases applid to the hidden layer when mapping to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first set of weights is stored in an array with one row per pixel (i.e., one row per input node in our network), and one column per hidden node.  Therefore, if we want to see what pattern the a hidden node is detecting, we pass the corresponding weight column to the digit plot function we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Which hidden node?\n",
    "hid_node = 450\n",
    "\n",
    "# Plot it\n",
    "# we're recylcing the function from before, so the title will still say\n",
    "# 'Training', but ignore that.\n",
    "plot_handwritten_digit(model.get_weights()[0][:,hid_node-1].reshape((28, 28)), hid_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.get_weights()[0][:,1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Network with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try a deeper neural network with more layers.  Specifically, one input layer, two fully connected hidden layers using sigmoid activation, and one output layer using softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tell keras we want to create a sequential (feed-forward network) model, in which one\n",
    "# layer follows the next\n",
    "deeper_model = Sequential()\n",
    "\n",
    "# Create the input layer and the hidden layer of the network\n",
    "# 'Dense' indicates that we want all inputs to connect to every node in the hidden layer\n",
    "# 'input_shape' tells the hidden layer the dimension of the input to expect, which is determined\n",
    "#    by our data (the number of predictors in our data set = 784 pixels per image)\n",
    "# 'NUM_HIDDEN_UNITS' (which we defined above) tells keras how many nodes are the hidden layer connected\n",
    "#    to the input layer\n",
    "# 'activation' specifies how values from the input node should be processed by hidden nodes.\n",
    "deeper_model.add(Dense(NUM_HIDDEN_UNITS, input_shape=(784,), activation='sigmoid', init='uniform'))\n",
    "\n",
    "# Next, implement dropout in the first layer, with 50% of the input nodes being \n",
    "# dropped for each iteration.\n",
    "deeper_model.add(Dropout(0.5))\n",
    "\n",
    "# Next, create a second hidden layer with sigmoid activation, and\n",
    "# implement drop out.  Keras will look at the most layer we just created\n",
    "# to figure out how many inputs to expect.\n",
    "deeper_model.add(Dense(NUM_HIDDEN_UNITS, activation='sigmoid', init='uniform'))\n",
    "deeper_model.add(Dropout(0.5))\n",
    "\n",
    "# Next, we'll create the output layer.\n",
    "deeper_model.add(Dense(10, activation='softmax', init='uniform'))\n",
    "\n",
    "# Next, we specify the properties of our optimizer.\n",
    "# We'll be using stochastic gradient descent with momentum, along with \n",
    "# the crossentropy cost function.\n",
    "# Note: We're using a larger learning rate than in the single-layer network, and\n",
    "# and no decay.  I arrived at these settings through tuning, but intuitively this is \n",
    "# a more complicated function to optimize (given the additional layer)\n",
    "# and since I don't want to run over a huge number of epochs, I'm asking keras\n",
    "# to learn faster.  This may not always work in practice.\n",
    "sgd = SGD(lr=4.*LEARNING_RATE, decay=0., momentum=MOMENTUM, nesterov=True)\n",
    "deeper_model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Summarize the model setup\n",
    "deeper_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the response once again, and train the network.\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "deeper_model.fit(X_train, Y_train, batch_size=BATCH_SIZE, nb_epoch=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the accuracy of this deeper network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_keras = deeper_model.predict(X_test)\n",
    "test_preds = np_utils.categorical_probas_to_classes(Y_test_keras)\n",
    "print classification_report(y_test, test_preds)\n",
    "print accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, especially since it's worse than the single layer neural network!  I played around with this, and found that turning decay off and going for 30 epochs, instead of 10, helps quite a bit.  Let's do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorflow\n",
    "\n",
    "Tensorflow is an open-source package provided by Google specifically designed for deep learning. In terms of what we've done so far in this notebook, it's comparable to theano in that it's a computational backend that can be paired with keras.  However, it can also be used directly (as can theano actually) to specify, train, and evaluate neural networks.  \n",
    "\n",
    "The basic idea behind tensorflow is to describe a network as a  'map' in which nodes correspond to computations (e.g., apply softmax function or sigmoid function), and the edges between nodes correspond to the flow of *tensors* (or multi-dimensional arrays) as they get processed via successive computations. Essentially, the tensors \"flow\" through the map, getting modified along the way.  \n",
    "\n",
    "Like theano, one key function of tensorflow is its ability to automatically calculate the derivatives required by gradient descent.  It figures out how all the parameters feed into the loss function, and calculates derivatives accordingly.\n",
    "\n",
    "In addition, tensorflow has many advanced features that we won't discuss.  For instance, it is designed to work well with GPU's, which have been found to be very good at performing the computations required by neural networks.  It can also be used for distributed processing, or even on smart phones.\n",
    "\n",
    "Instructions for installing tensorflow can be found here: https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html.  If you want the simplest possible install, I'd stick with the 'CPU Only' approach, which avoids having to setup additional packages required for GPU processing.\n",
    "\n",
    "Also, there now exists a tensorflow wrapper in R, which let's you use tensorflow without leaving R. I have not tried it, but if you do, let me know what you think.\n",
    "https://rstudio.github.io/tensorflow/\n",
    "\n",
    "Note that the R wrapper warns against installing tensorflow using anaconda, which is unfortunate.  I used the conda installation for use in Python, which worked fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now do the digit classification problem in tensorflow.  This example is heavily based on Google's own example: https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/tutorials/mnist/mnist_softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull down the data in a tensforflow-friendly format\n",
    "mnist_tf = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data again consists of 28x28 pixel images, for 784 total pixes per image.  We therefore create a tensor flow *placeholder* to reprsent the input layer in our network.  We won't actually provide data yet -we're just telling tensorflow to expect data of a certain shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None,784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```float32``` tells tensorflow the type of data that will be stored in ```x```.  The ```[None,784]``` tells tensorflow to expect data with some (unspecified) number of rows, and 784 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to add objects to our tensorflow map that correspond to the weights and biases of our neural network.  Specifically, we create tensorflow ```variables```, which can be updated as tensforflow performs gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights and biases used to map inputs to hidden layer\n",
    "W = tf.Variable(tf.zeros([784,500]))\n",
    "b = tf.Variable(tf.zeros([500]))\n",
    "\n",
    "# Calculation of hidden layer using sigmoid activation\n",
    "h = tf.sigmoid(tf.matmul(x,W)+b)\n",
    "\n",
    "# Weights and biases used to map hidden layer to output layer\n",
    "hW = tf.Variable(tf.zeros([500,10]))\n",
    "hb = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Calculation to map hidden layer to output layer\n",
    "# Note: We'll apply softmax implicitly in the cost function\n",
    "y = tf.matmul(h,hW)+hb\n",
    "\n",
    "# Placeholder for the training response data\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our objective function, and how we'd like to update our parameters -in our case, via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y,y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.9).minimize(cross_entropy)\n",
    "#train_step = tf.train.MomentumOptimizer(learning_rate=0.5,momentum=0.9).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initialize all of the variables in the tensorflow map, start a session (tensorflow operations need to take place within a session), then iterate using the gradient descent algorithm in order to adjust the weights and minimize cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist_tf.train.next_batch(600)\n",
    "    sess.run(train_step,feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lastly, we calculate accuracy of our fitted neural network\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy,feed_dict={x: mnist_tf.test.images, y_: mnist_tf.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely bad, but I did no tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close the tensorflow session\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
